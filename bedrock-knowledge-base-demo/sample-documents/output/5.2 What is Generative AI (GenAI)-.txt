# 5.2 What is Generative AI (GenAI)-

So let's talk about Generative AI,just before we go into Amazon Bedrock,which is a service for Gen AI on AWS.But I first wanna take a step back,and just understand what is Gen AI?So Generative AI or Gen AIis going to be a subset of deep learning,which is a subset of machine learning,which is a subset of AI.So Gen AI, as the name indicates,is used to generate new datathat is going to be similarto the data it was trained on.So what type of data can we train a Gen AI with?Well, we can train it on text,on images, on audio,on code and video, and a lot more,whatever you think of really.So the Generative AI modelis going to take a lot of trained data.Here's an example where we are going to give ita lot of dogs images.So we have our trained data set,which are going to be dogs,but also we are going to feed them cartoons.So we're gonna have a bunch of different cartoonsthat are going to be hand drawn.And the generative model is going to see so manyof these trained datathat is going to understand what is a dog,what is a cartoon,and then if we ask it,"Can you generate a cartoon dog?"It's going to be smart enough to combine the two togetherand create a dog that looks like a cartoon.And that is the whole power of Generative AI,is that it's able to combine its knowledgeinto new and unique ways.So, we're going to start with a lot of unlabeled data,and we'll see that means in the later section.And we're going to train what's called a foundation model.And foundation model are very broad,they're very big, very wide,and they can adapt to different kind of general tasks.For example, a good foundation model can generate some text,can summarize some texts,can extract information,can generate images,can become a chatbot,and can answer any types of questions you have.So as a whole,we feed a lot of data into a foundational model,which has the option to do a lot of different tasks.So now let's talk about foundation models.So in order to generate data, as we said,we need to have a foundation model,and they're trained on variety of inputs.But to let you know how big these models are,usually to train a foundation model, a good one,it may cost tens of millions of dollars to train,because it is very computational heavy.It takes a lot of time to train it, and a lot of data.So only a few big companies, usually,are creating their own foundation model.So here is an example.So we have GPT-4o,that is the name of the foundation model behind ChatGPT,which is the application where you can chat with an AI.But there is a wide selection of foundation modelsfrom different companies.So we have OpenAI,this is the company behind ChatGPT, and GPT-4o for example.We have Meta, so this is the new company behind Facebook.We have Amazon, Google, and Anthropic,and of course a lot more.But these companies are pretty big,or they have a lot of money to invest into buildingthese foundation models.So some of these models are going to be open source,for example, they're going to be free.For example, Meta is working a lot on open source models.We have Google BERT as well,which was one of the first models in the Gen AI space,which also is open source.But some are under a commercial license,for example, OpenAI you have to pay to use GPTat a certain level.Anthropic and so on.And so we're going to seehow we can access these models on AWS as well.So next, after the foundation model,we have the Large Language Models or LLM.So LLMs are a type of AIthat is relying again on a foundation model,but designed to generate coherent human-like text.So you've been exposed to LLMs a lot before.One of these is ChatGPT,the GPT-4 type of foundation model,an LLM from the company OpenAI.So I went to ChatGPT and I asked it,"Are you an LLM?"And it replied, "Yes, I am a Large Language Modeldeveloped by OpenAI,and I can understand and generate human-like textbased on the input I receive."So this was text that looks like a human wrote it,that was answered by ChatGPT when I asked it,"Are you an LLM?"So the way LLM works is that they're trainedon very large amount of text data.So they're usually very, very, very big models,very heavy, very computational heavy to use.We're talking about billions of parameters.They're trained on a lot of books, articles,websites, data, or any other type of text datathat is deemed good enough for the LLM training.So it can perform any wide range of language related taskssuch as translation, summarization,question answering, content creation, and so on.So how does it work to use an LLM?So for this, we give it a prompt.So the prompt is a question,a bunch of texts that you're going to sendto the Gen AI model, the LLM,for example, "What is AWS?"That's the prompt.And we'll have a whole section in this courseto understanding how to create a good prompt.Then the model is going to leverageall the existing content it has internally,learned from, and then it's going to look at the promptand answer it.And so when I asked ChatGPT, "What is AWS?"I get the answer,"AWS is a comprehensive cloud computings platformprovided by Amazon."And you can read the rest, it's a long answer.But you have to know something,and it's a term you have to learn,which is that the output, the generated text,is non-deterministic.That means that for every userthat is using the same prompt,you will not necessarily,and usually not, get the same answer.So I went a second time and opened a new chat windowand asked, "What is AWS?"And if you take some time to read this answer,this was against ChatGPT,you will see that while the answers are similar,and they explain the same thing, pretty much,they are not the same exact answers.And so this is why it's non-deterministic.So let's understand why though it is non-deterministic.So let's take a sentence that is going to be,I'll put it by a LLM,and the sentence is,"After the rain, the streets were."And what's going to happen is that the LLMis going to generate a list of potential wordswith probabilities.So what is the next wordthat is going to be probably here in this sentence?So the generative model is going to think,and it's gonna say, okay, maybe it's wet,and there's a 0.4 out of one chancethat it's going to be wet.Or flooded, 0.25.Slippery, 0.15.Empty, muddy, clean,blocked, and so on.So all these sentences make sense,but there are probabilitiesthat means that some of these words are more likelyto be the next word in that sentence.And an algorithm is going to of course,compute these probabilities,and another one is going to select a word from that listbased on the probabilities.And for example,we're going to choose the word, flooded.So it's going to be,"After the rain, the streets were flooded."And this is all done by the Gen AI model.So now we have, "After the rain, the streets were flooded,"and the same process happens over and over again.So what is the next word?Well, it could be and, with, but, from, until, because,and even a dot.So "After the rain, the streets were flooded dot,"and that's the end of the sentence.So all of these things, again,have associated probabilities.And then the next word is going to be selectedbased on these probabilities.So this is why when you ask the AI twice the same prompt,you may not get the same answers,it is because the sentence is determinedthanks to statistical methods,and not with deterministic methods.So that's for LLMs,but let's talk about images as well.So Gen AI for images works in a way that, for example,you can give it a prompt,for example, generate a blue sky with white clouds,and the word "Hello" written in the sky.And the Gen AI model is going toactually give you that imagethat of course we generated for this course.We can also have images generated from images.So here we give an image of someone playing piano,and we're saying,"Transform this image in a Japanese anime style."And the outcome image is going to be something similar,but now it looks like it comes out of a manga.And then we can also generate text from images.So we give it a prompt and say,"How many apples do you see in the picture?"And we give it a picture with one orange and an apple.And then the Gen AI is going to look at the image and say,"Well, the picture shows one appleand the other fruit is an orange."So just to tell you how that works for Gen AI for images,so you get an idea of how something can generate an image,there's different methods of course,but one of the popular one nowadaysis the diffusion modelfrom a company, for example, named Stable Diffusion,which is using that model heavily.So let's take a picture,and this is a picture of a cat.And we're going to do what's calleda forward diffusion process.That means that we're going to addsome noise to the image over time.So this is with a little bit of noise,but it's the same image with a bit of noise.And then we add more noise, we can barely recognize the cat,and then we add more noise,and it looks like the cat is entirely gone,and all we get is noise.And so we do this for a lot of pictures,and this is called the forward diffusion process.And once the algorithm is trainedto take images and create noise out of it,we do the opposite.So when we want to generate an image,we're going to start with noise,and we're also going to give it a prompt and say,"We want a cat with a computer,"and it's called reverse diffusion.So now that the algorithm has seen how to gofrom an image to noise,it will go from noise to image.So we give it some noise, randomly,and then it says, "Okay, I'm going to de-noise it,"and it's going to start to look like a cat.And then de-noise it again, and then de-noise it again,and then we have the cat with a computer.So imagine this is a new image generated by the AI,and not the exact same it was trained on, of course.So this is how Gen AI works for texts, for images.Just remember the concept of an LLM,remember the concept that it's non-deterministic.But now you have a general overview of Gen AI,and I hope you liked it,and I will see you in the next lecture.