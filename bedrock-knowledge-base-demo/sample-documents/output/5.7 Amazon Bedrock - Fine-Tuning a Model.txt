# 5.7 Amazon Bedrock - Fine-Tuning a Model

So now let's talk about fine-tuningon Amazon Bedrock.So fine-tuning is going to be a big part of your exam.So the idea is that you're going to adapt a copyof a foundation model and you're going to add your own data.So when you fine-tune a model,it's actually going to change the underlying weightof the base foundation model.So you need to provide training dataand it needs to adhere to a specific formatand needs to be stored into Amazon S3.So the idea is that, for example,you have the LLAMA 2 modeland you're going to add data from Amazon S3,such as this data or that data,and we'll have a look at this datavery specifically in the next slides.And then Bedrock is going to do its own thingand you're going to get a fine-tuned version of LLAMA 2,which has your own data as well.So in order to use a fine-tuned custom model,you must use what's called provisioned throughput,which is a different pricing model than on-demand.And note that not all models can be fine-tuned,but few can and they're usually open source.So how can we fine-tune a model?Well, we have instruction-based fine-tuning.And here, this is to improve the performanceon the pre-trained foundation modelon domain-specific tasks.So what does it mean domain-specific tasks?This is something you'll see at the exam.That means it's going to be further trainedon a particular field or area of knowledge.And here, the trick you need to go and look for at the examis that instruction-based fine-tuningis going to use what's called labeled examplesand they're going to be prompt-responses pairs.So it's for labeled dataand the prompt-response pairs, look at this,the prompt is "Who is Stephane Maarek" for example,and the completion.So the response is "Stephane Maarek is an AWS instructorwho dedicates his time to make the best AWS coursesso that his students can pass all certificationswith flying colors!"So here, on top of giving information to the model,we are also showing the model how we want itto order some questions such as, "Who is Stephane Maarek?"Maybe the answer the model had would be already similar,but with a different tone.So this is where instruction-based fine-tuning is helpful.Next, we have continued pre-training.So here the idea is that we continue the trainingof the foundation model.So here, because we know foundation model have been trainedusing unlabeled data, we need to provide,as well for continued pre-training, unlabeled data.And so this is something you look for in the exam.If you have unlabeled data,this is a kind of fine-tuning you need.And so it's also called domain-adaptation fine-tuning,to make a model an expert in a specific domain.For example, I'm going to feedthe entire AWS documentation to a model,and then the model is going to be an expert on AWS.So here we're just giving all documentation,it is unlabeled data so this is continued pre-training.And now the domain,the model has become a domain expert.So here's the kind of input you have here.As you see, there's no prompt output,there's just input.And the input is just a lot of information.So here this information is around financial data.And as you can see, if you wanna read through it,it has a lot of acronyms.And so this is very good to teach acronymsor feed industry-specific terminology into a model.And then you can keep on training the model,it's called continued pre-training,as more data becomes available.Okay, next you may encounter as well single-turn messagingand multi-turn messaging.So this is a subset of instruction-based fine-tuning.But the idea is that, here,we're going to give a hint into a user and an assistantwhat the user is asking and what the assistant,so the bot, should be replying.So here we have system,this is optional context for the conversation.Messages, which is going to contain various messages.Each will have a role, which is the user of the assistant.And the content, which is the text content of the message.So here we're fine-tuning how a chat bot should be replying.And for multi-term messaging, this is the same idea,but this time we have a conversationso we have multiple turns.And so here we alternate between user and assistant roles,and we have a conversation.And this helps the model understand howto handle conversations with bigger context.So good to know for fine-tuning.First of all, re-training a foundational modelrequires a higher budgetbecause you need to spend some computations on it.So instruction-based fine-tuningis usually going to be cheaperbecause the computations are less intenseand, usually, you have less data required.You're just trying to fine-tunehow the model is replying based on specific instructions.If you use continued pre returning,it's usually more expensivebecause you need to have a lot more data.Also, it requires you to have an experiencedmachine learning engineer to perform the task,even though Bedrock makes it easy for you.And you must prepare the data,you must do the fine-tuning,and also evaluate the model.And finally, because you have a fine-tuned model,it's also more expensivebecause you have to use provisioned throughput.So now let's talk about transfer learning.So transfer learning is a bit broader than fine-tuning.It is the concept of using a pre-trained modelto adapt it to a new related task.For example, we have Claude 3and then we're going to do transfer learningto adapt it to a new task.So you say maybe it's very similar to fine-tuning,and it is, but for example, for image classification,we may want to use a pre-trained modelthat knows how to recognize edges and images,but we may want to do transfer learningto apply it to recognize specifically a kind of image.Or for language processing type of models,for example BERT or GPT,again, they know how to process the language.So now that we have the language figured out,let's just fine-tune themor use transfer learning to adapt it to newer tasks.So transfer learning is in this lecturebecause it can appear in the examas a general machine learning concept that will be used,for example, to, as the definition says,adapt a model to a new task.So if you don't see fine-tuning,just know that the general answeris to use transfer learningbecause fine-tuning is a specific kind of transfer learning.So the use cases of fine-tuning is, for example,to have this chatbot designedwith a particular persona or tone,or geared towards a specific purposesuch as existing customer or crafting advertisements.It's also to have trained more up to datethan what the model previously accessed.Also is to train it with exclusive data that you have only.For example, historical emails or messagesor records for customer service interaction.Of course, base foundation models do not have access to thisbecause this is your data.And for targeted use casessuch as categorization or assessing accuracy.So let's say we're fine-tuning,the exam will ask you about when fine-tuning is a good ideaand the kind of fine-tuning you will needbased on the type of data you get,for example labeled or unlabeled data,as well as maybe some pricing questions.All right, that's it.I hope you liked it and I will see you in the next lecture.