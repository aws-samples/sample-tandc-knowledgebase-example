# 5.8 Amazon Bedrock - FM Evaluation

So in order to choose a model,sometimes you may want to evaluate that modeland you may want to bring some level of rigorwhen you evaluate that model.So you can do on Amazon Bedrockwhat's called Automatic Evaluation.So this is to evaluate a model for quality controland then you're going to give it some tasks.So you have some built-in task typessuch as, for example, exercises on text summarization,question and answer, text classification,or open-ended text generation.And so you're going to choose one of these text typesand then you need to add a prompt datasetsor you can use oneof the built-in, curated prompt datasetsfrom AWS on Amazon Bedrock.And then thanks to all this,scores are going to be calculated automatically.So let me show you what I mean in a diagramso you really understand what happens.So we have benchmark questionsand again, you can bring your own benchmark questionsor you can use the one from AWS.And then of course, you have questions,but because you've created benchmark,you need to have benchmark questions,as well as benchmark answers,and the benchmark answers are what would be for youan ideal answer to your benchmark question.Then you have the model to evaluateand you're going to submit all the benchmark questionsinto the model that must be evaluatedwhich is going to of course, generate some answersand these answers are generated by a GenAI model.And then of course, we need to compare the benchmark answersto your generated answers.So we compare these twoand because we are in an automatic evaluation,then it's going to be another model, another GenAI model,called a judge modelwhich is going to look at the benchmark answerand generate an answerand is going to be asked something along the lineof can you tell if these answers are similar or not?And then it is going to give a grading scoreand there are different waysto calculate this grading score.For example, the BERTScore or the F1 or so on,but no need to linger on that specific jargon for now.So a quick note on benchmark datasets.So they're very helpfuland a benchmark datasetis a curated collection of datadesigned specifically to evaluatethe performance of a language modeland it can cover many different topics, or complexities,or even linguistic phenomena.So why do you use benchmark datasets?Well, they're very helpfulbecause you can measure the accuracy of your model,the speed and efficiency,and the scalability of your modelbecause you may throw a lot of requests at itat the same time.So some benchmark datasets are designedto allow you to quickly detect any kind of biasand potential discrimination against a group of peoplethat your model may make,and this is something the exam can ask you.And so therefore using a benchmark datasetgives you a very quick, low administrative effortto evaluate your models for potential bias.Of course, it is possible for youto also create your own benchmark datasetsthat are going to be specific to your businessif you need to have specific business criteria.Of course, we can do also human evaluations.So this is the exact same idea.We have benchmark questions and benchmark answers,but then some humans,employees, for example, from the work team,could be employees of your companyor it could be subject matter expertsor SME or whatever,are going to look at the benchmark answersand the generated answers,and they're going to sayokay, this looks correct or not correct.So how can they evaluate?Well, there's different type of metrics.There's thumbs up or thumbs down,there's ranking and so on,and then it's going to give a grading score again.So this time there's a human part in itand you may prefer it.You can again choose from the built-in task typesor you can create a custom taskbecause now humans are evaluating itso you are a little more free.So there are a few metrics you can useto evaluate the output of an FM from a generic perspective.We have the ROUGE, the BLEU, the BERTScore, and perplexityand I'm going to give you a high level overview,so we get you understand themand they should be more than enough for the exam.So rouge is calledRecall-Oriented Understudy for Gisting Evaluation.So here the purpose of it,and I think that's what you need to understandfrom a exam perspective,is to evaluate automatic summarizationand machine translation system.So very dedicated to these two thingsand we have a different kind of metrics.We have ROUGE-N, and N can changebetween one, two, three, four usually,used to measure the number of matching n-gramsbetween reference and generated text.So what does that mean?That means you have a reference text,this is what you would like the output to beof your foundation model,and then whatever text has been generatedby the foundation model.And ROUGE is going to look at how many n-grams are matching.So if you take a one-gram,that means how many words are matchingbecause a one-gram is just a word.But if you take two-grams,that means that it's a combination of two words.So if you have the apple fell from the tree,you're going to look at the apple, apple fell,fell from, from the, and the tree,and again, you look at how many matchesbetween your reference text and you generate a task.If you take a very high gram, for example, 10-grams,it means you have 10 wordsmatching exactly in the same orderfrom one reference to the generated text.But it's a very easy one to computeand very easy one to make sense of.And you have a ROUGE-Lwhich is going to compute the longest common subsequencebetween reference and generated text.What is the longest sequence of wordsthat is shared between the two texts?Which makes a lot of sense,for example, if you have machine translation systems.Then you have BLEU.So ROUGE, by the way, is red in Frenchand BLEU is blue in French,so just have some colors.Blue is Bilingual Evaluation Understudy.So here this is to evaluatethe quality of generated text, especially for translation.So this is for translationsand it considers both precisionand is going to penalize as well for too much brevity.So it's going to look at a combination of n-grams.The formula is a little bit different,but if the translation is too short, for example,it's going to give a bad score.So it's a slightly more advanced metricand I'm not going to show the mechanism underneathbecause you don't need to know it,but it's very helpful for translationsand you need to remember it.But these two things, ROUGE and BLEU,they just look at words, combination of words,and they look at the comparison.But we have something a bit more advanced.Now because of AI,we have the BERTScore.So here we lookfor the semantic similarity between generated text.What does that mean?That means that you're going to comparethe actual meaning of the textand see if the meanings are very similar.So how do we do meaning?Well, you're going to have a modeland it's going to compare the embeddings of both the texts,and it can compute the cosine similarity between them.So embeddings are something we'll see very, very soonand they're way to look at a bunch of numbersthat represent the text.And if these numbers are very close between two embeddings,then that means the textsare going to be semantically similar.And so here with the BERTScore,we're not looking at individual words.We're looking at the contextand the nuance between the text.So it's a very good one nowbecause we have access to AI.And perplexity is how well the modelwill predict the next token,so lower is better, and that means that if a modelis very confident about the next token,that means that it will be less perplexedand therefore more accurate.So just to give you a diagram.Here we have a generative AI modelthat we trained on clickstream data, card data,purchase items, and customer feedbackand we're going to generate dynamic product descriptions.And so from this,we can use the reference one versus the one generatedto compute the ROUGE or the BLEU metric,as well as also look at some similarityin terms of nuance with a BERTScore.And all these things can be incorporatedback into a feedback loopto make sure we can retrain the modeland get better outputsbased on the quality of the scores of these metrics.On top of just having these type of gradingof a foundation model,you may have business metrics to evaluate a model onand these are a little bit more difficultto evaluate, of course,but it could be user satisfaction.So you gather user feedbackand you assess the satisfaction within the model response,so for example, the user satisfactionof an e-commerce platform,or you can compute what is the average revenue per user,and of course, well, if the GenAI app is successful,you hope that this metric will go up.Or cross-domain performance, so is the model able to performacross a varied tasks across different domains?Conversion rates, so what is the outcome I want?Do I want to have higher conversion rates?Again, I would monitor thisand evaluate my model on that.Or efficiency, what is the efficiency of the model?How much does it cost me?Is it efficient in competition,in resource utilization, and so on?So that's it for evaluating a foundation model.I hope you like itand I will see you in the next lecture.